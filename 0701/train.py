
import os
import pandas as pd
from PIL import Image
import torch
from torch import nn
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms, models
from tqdm import tqdm
import torch.nn.functional as F
from sklearn.model_selection import train_test_split
import numpy as np
from datetime import datetime, timedelta
import json

# ============= í”„ë¡œì íŠ¸ ê²½ë¡œ ì„¤ì • =============
PROJECT_ROOT = "C:/planttest0/"
CSV_PATH = os.path.join(PROJECT_ROOT, "0701", "dataset8_3.3_updated.csv")
IMG_DIR = "C:/planttest0/TS/TS_3.3/TS_3.3"

BATCH_SIZE = 16
EPOCHS = 3
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# ìƒì¶” ì„±ì¥ ìƒìˆ˜
LETTUCE_GROWTH_STAGES = {
    "ì •ì‹ê¸°": (0, 7),      # 0-7ì¼: ì‹¬ì€ ì§í›„
    "ìƒìœ¡ê¸°": (7, 21),     # 7-21ì¼: ì£¼ìš” ì„±ì¥ê¸°
    "ìˆ˜í™•ê¸°": (21, 35)     # 21-35ì¼: ìˆ˜í™• ê°€ëŠ¥
}

# ============= ë©€í‹°ëª¨ë‹¬ ë°ì´í„° ì „ì²˜ë¦¬ =============
def analyze_multimodal_data(csv_path):
    """ì´ë¯¸ì§€, ë‚ ì§œ, ë©”íƒ€ë°ì´í„°ë¥¼ ì¢…í•© ë¶„ì„"""
    print("ğŸ” ë©€í‹°ëª¨ë‹¬ ë°ì´í„° ë¶„ì„ ì¤‘...")
    
    df = pd.read_csv(csv_path)
    print(f"ğŸ“Š ì´ ë°ì´í„°: {len(df)}ê°œ")
    print(f"ğŸ“‹ ì»¬ëŸ¼: {list(df.columns)}")
    
    # 1. ë‚ ì§œ ì •ë³´ ì²˜ë¦¬
    if 'planting_date' in df.columns:
        df['planting_date'] = pd.to_datetime(df['planting_date'])
        df['photo_date'] = pd.to_datetime(df['photo_date']) if 'photo_date' in df.columns else datetime.now()
        df['days_since_planting'] = (df['photo_date'] - df['planting_date']).dt.days
    else:
        print("âš ï¸ ì‹¬ì€ ë‚ ì§œ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤. estimated_days_elapsed ì‚¬ìš©")
        df['days_since_planting'] = df['estimated_days_elapsed']
    
    # 2. ë‚ ì§œ ê¸°ë°˜ ì„±ì¥ ë‹¨ê³„ ì˜ˆì¸¡
    def predict_stage_by_date(days):
        if days <= 7:
            return 0, "ì •ì‹ê¸°"
        elif days <= 21:
            return 1, "ìƒìœ¡ê¸°"
        else:
            return 2, "ìˆ˜í™•ê¸°"
    
    df['predicted_stage_by_date'] = df['days_since_planting'].apply(lambda x: predict_stage_by_date(x)[0])
    df['predicted_stage_name'] = df['days_since_planting'].apply(lambda x: predict_stage_by_date(x)[1])
    
    # 3. ì‹¤ì œ ë¼ë²¨ê³¼ ë‚ ì§œ ì˜ˆì¸¡ ë¹„êµ
    stage_map = {"ì •ì‹ê¸°": 0, "ìƒìœ¡ê¸°": 1, "ìˆ˜í™•ê¸°": 2}
    df['actual_stage'] = df['growth_stage'].map(stage_map)
    
    # ë‚ ì§œ vs ì‹¤ì œ ë¼ë²¨ ì¼ì¹˜ë„ ë¶„ì„
    df['stage_consistency'] = (df['predicted_stage_by_date'] == df['actual_stage']).astype(int)
    consistency_rate = df['stage_consistency'].mean()
    print(f"ğŸ“… ë‚ ì§œ-ë¼ë²¨ ì¼ì¹˜ë„: {consistency_rate:.2%}")
    
    # 4. ìˆ˜í™•ê¹Œì§€ ë‚¨ì€ ì¼ìˆ˜ (ë” ì •í™•í•œ ê³„ì‚°)
    df['days_to_harvest'] = np.maximum(0, 30 - df['days_since_planting'])
    
    print(f"âœ… ë°ì´í„° ì „ì²˜ë¦¬ ì™„ë£Œ!")
    print(f"ğŸ“Š ì„±ì¥ ë‹¨ê³„ë³„ ë¶„í¬:")
    print(df['actual_stage'].value_counts().sort_index())
    
    return df

# ============= ë©€í‹°ëª¨ë‹¬ ë°ì´í„°ì…‹ =============
class MultimodalLettuceDataset(Dataset):
    """ì´ë¯¸ì§€ + ë‚ ì§œ + ë©”íƒ€ë°ì´í„°ë¥¼ ê²°í•©í•œ ë°ì´í„°ì…‹"""
    
    def __init__(self, dataframe, img_dir, transform=None):
        self.data = dataframe.reset_index(drop=True)
        self.img_dir = img_dir
        self.transform = transform
        
        # ìœ íš¨í•œ ì´ë¯¸ì§€ë§Œ í•„í„°ë§
        valid_indices = []
        for idx in range(len(self.data)):
            fname = self.data.iloc[idx]['fname']
            img_path = os.path.join(img_dir, fname)
            if os.path.exists(img_path):
                valid_indices.append(idx)
        
        self.data = self.data.iloc[valid_indices].reset_index(drop=True)
        print(f"ğŸ“· ìœ íš¨í•œ ë°ì´í„°: {len(self.data)}ê°œ")

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        row = self.data.iloc[idx]
        
        # 1. ì´ë¯¸ì§€ ë¡œë“œ
        img_path = os.path.join(self.img_dir, row["fname"])
        try:
            image = Image.open(img_path).convert("RGB")
        except Exception as e:
            print(f"âš ï¸ ì´ë¯¸ì§€ ë¡œë“œ ì˜¤ë¥˜: {row['fname']}")
            print(f"   ê²½ë¡œ: {img_path}")
            print(f"   íŒŒì¼ ì¡´ì¬: {os.path.exists(img_path)}")
            print(f"   ì—ëŸ¬: {str(e)}")
            image = Image.new('RGB', (224, 224), color=(34, 139, 34))
        
        if self.transform:
            image = self.transform(image)
        
        # 2. ë‚ ì§œ ì •ë³´ (ì •ê·œí™”)
        days_since_planting = row['days_since_planting'] / 35.0  # 0-1 ë²”ìœ„ë¡œ ì •ê·œí™”
        days_to_harvest = row['days_to_harvest'] / 35.0
        
        # 3. ë©”íƒ€ë°ì´í„° í”¼ì²˜
        metadata_features = torch.tensor([
            days_since_planting,
            days_to_harvest,
            row['predicted_stage_by_date'] / 2.0,  # 0-1 ë²”ìœ„ë¡œ ì •ê·œí™”
            row['stage_consistency'],
        ], dtype=torch.float32)
        
        # 4. íƒ€ê²Ÿ ê°’ë“¤
        actual_stage = torch.tensor(row['actual_stage'], dtype=torch.long)
        days_left = torch.tensor([row['days_to_harvest']], dtype=torch.float32)
        
        return image, metadata_features, actual_stage, days_left

# ============= ë©€í‹°ëª¨ë‹¬ ëª¨ë¸ =============
class MultimodalLettuceModel(nn.Module):
    """ì´ë¯¸ì§€ + ë©”íƒ€ë°ì´í„°ë¥¼ ê²°í•©í•œ ë©€í‹°ëª¨ë‹¬ ëª¨ë¸"""
    
    def __init__(self, num_stages=3, metadata_dim=4):
        super().__init__()
        
        # 1. ì´ë¯¸ì§€ ì¸ì½”ë” (Vision)
        self.vision_encoder = models.mobilenet_v3_small(weights='DEFAULT')
        self.vision_features = self.vision_encoder.features
        self.vision_avgpool = self.vision_encoder.avgpool
        
        # 2. ë©”íƒ€ë°ì´í„° ì¸ì½”ë” (ë‚ ì§œ, í™˜ê²½ ì •ë³´)
        self.metadata_encoder = nn.Sequential(
            nn.Linear(metadata_dim, 32),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(32, 64),
            nn.ReLU(),
            nn.Dropout(0.1)
        )
        
        # 3. ë©€í‹°ëª¨ë‹¬ ìœµí•© ë ˆì´ì–´
        vision_dim = 576  # MobileNetV3-small ì¶œë ¥ í¬ê¸°
        fusion_dim = vision_dim + 64
        
        self.fusion_layer = nn.Sequential(
            nn.Linear(fusion_dim, 256),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(256, 128),
            nn.ReLU(),
            nn.Dropout(0.1)
        )
        
        # 4. íƒœìŠ¤í¬ë³„ í—¤ë“œ
        self.stage_head = nn.Linear(128, num_stages)      # ì„±ì¥ ë‹¨ê³„
        self.days_head = nn.Linear(128, 1)                # ë‚¨ì€ ì¼ìˆ˜
        
        # 5. ì–´í…ì…˜ ë©”ì»¤ë‹ˆì¦˜ (ë¹„ì „ vs ë©”íƒ€ë°ì´í„° ê°€ì¤‘ì¹˜)
        self.attention = nn.Sequential(
            nn.Linear(fusion_dim, 64),
            nn.ReLU(),
            nn.Linear(64, 2),  # vision_weight, metadata_weight
            nn.Softmax(dim=1)
        )

    def forward(self, image, metadata):
        batch_size = image.size(0)
        
        # 1. ì´ë¯¸ì§€ íŠ¹ì§• ì¶”ì¶œ
        vision_features = self.vision_features(image)
        vision_features = self.vision_avgpool(vision_features)
        vision_features = torch.flatten(vision_features, 1)
        
        # 2. ë©”íƒ€ë°ì´í„° íŠ¹ì§• ì¶”ì¶œ
        metadata_features = self.metadata_encoder(metadata)
        
        # 3. íŠ¹ì§• ê²°í•©
        combined_features = torch.cat([vision_features, metadata_features], dim=1)
        
        # 4. ì–´í…ì…˜ ê°€ì¤‘ì¹˜ ê³„ì‚°
        attention_weights = self.attention(combined_features)
        
        # 5. ê°€ì¤‘ì¹˜ ì ìš©í•œ íŠ¹ì§• ìœµí•©
        weighted_vision = vision_features * attention_weights[:, 0:1]
        weighted_metadata = metadata_features * attention_weights[:, 1:2]
        
        # 6. ìµœì¢… íŠ¹ì§•
        final_features = self.fusion_layer(
            torch.cat([weighted_vision, weighted_metadata], dim=1)
        )
        
        # 7. ì˜ˆì¸¡
        stage_pred = self.stage_head(final_features)
        days_pred = self.days_head(final_features)
        
        return stage_pred, days_pred, attention_weights

# ============= í•™ìŠµ í•¨ìˆ˜ =============
def train_multimodal_model():
    """ë©€í‹°ëª¨ë‹¬ ëª¨ë¸ í•™ìŠµ"""
    
    print("ğŸš€ ë©€í‹°ëª¨ë‹¬ ìƒì¶” ì˜ˆì¸¡ ëª¨ë¸ í•™ìŠµ ì‹œì‘!")
    
    # 1. ë°ì´í„° ë¡œë“œ
    df = analyze_multimodal_data(CSV_PATH)
    
    # 2. í•™ìŠµ/ê²€ì¦ ë¶„í• 
    train_df, val_df = train_test_split(
        df, test_size=0.2, random_state=42, 
        stratify=df['actual_stage']
    )
    
    print(f"ğŸ“š í•™ìŠµ ë°ì´í„°: {len(train_df)}ê°œ")
    print(f"ğŸ” ê²€ì¦ ë°ì´í„°: {len(val_df)}ê°œ")
    
    # 3. ë°ì´í„° ë³€í™˜
    train_transform = transforms.Compose([
        transforms.Resize((256, 256)),
        transforms.RandomCrop(224, padding=4),
        transforms.RandomHorizontalFlip(0.5),
        transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.2, hue=0.1),
        transforms.RandomRotation(15),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])
    
    val_transform = transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])
    
    # 4. ë°ì´í„°ì…‹ ìƒì„±
    train_dataset = MultimodalLettuceDataset(train_df, IMG_DIR, train_transform)
    val_dataset = MultimodalLettuceDataset(val_df, IMG_DIR, val_transform)
    
    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)
    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)
    
    # 5. ëª¨ë¸ ì´ˆê¸°í™”
    model = MultimodalLettuceModel().to(DEVICE)
    
    # 6. ì†ì‹¤ í•¨ìˆ˜ ë° ì˜µí‹°ë§ˆì´ì €
    criterion_stage = nn.CrossEntropyLoss()
    criterion_days = nn.MSELoss()
    
    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)
    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS)
    
    # 7. í•™ìŠµ ë£¨í”„
    best_val_loss = float('inf')
    
    for epoch in range(EPOCHS):
        # í•™ìŠµ
        model.train()
        train_loss = 0
        train_acc = 0
        
        pbar = tqdm(train_loader, desc=f"ğŸŒ± Epoch {epoch+1}/{EPOCHS}")
        for images, metadata, stages_true, days_true in pbar:
            images = images.to(DEVICE)
            metadata = metadata.to(DEVICE)
            stages_true = stages_true.to(DEVICE)
            days_true = days_true.to(DEVICE)
            
            # ìˆœì „íŒŒ
            stages_pred, days_pred, attention_weights = model(images, metadata)
            
            # ì†ì‹¤ ê³„ì‚°
            loss_stage = criterion_stage(stages_pred, stages_true)
            loss_days = criterion_days(days_pred, days_true)
            
            # ê°€ì¤‘ ì†ì‹¤ (ì„±ì¥ ë‹¨ê³„ê°€ ë” ì¤‘ìš”)
            total_loss = 0.7 * loss_stage + 0.3 * loss_days
            
            # ì—­ì „íŒŒ
            optimizer.zero_grad()
            total_loss.backward()
            optimizer.step()
            
            # í†µê³„
            train_loss += total_loss.item()
            _, predicted = torch.max(stages_pred.data, 1)
            train_acc += (predicted == stages_true).sum().item() / len(stages_true)
            
            pbar.set_postfix({
                'Loss': f"{train_loss/len(pbar):.3f}",
                'Acc': f"{train_acc/len(pbar):.3f}"
            })
        
        # ê²€ì¦
        model.eval()
        val_loss = 0
        val_acc = 0
        
        with torch.no_grad():
            for images, metadata, stages_true, days_true in val_loader:
                images = images.to(DEVICE)
                metadata = metadata.to(DEVICE)
                stages_true = stages_true.to(DEVICE)
                days_true = days_true.to(DEVICE)
                
                stages_pred, days_pred, attention_weights = model(images, metadata)
                
                loss_stage = criterion_stage(stages_pred, stages_true)
                loss_days = criterion_days(days_pred, days_true)
                total_loss = 0.7 * loss_stage + 0.3 * loss_days
                
                val_loss += total_loss.item()
                _, predicted = torch.max(stages_pred.data, 1)
                val_acc += (predicted == stages_true).sum().item() / len(stages_true)
        
        # í‰ê·  ê³„ì‚°
        train_loss /= len(train_loader)
        val_loss /= len(val_loader)
        train_acc /= len(train_loader)
        val_acc /= len(val_loader)
        
        scheduler.step()
        
        print(f"\nğŸ“Š Epoch {epoch+1} ê²°ê³¼:")
        print(f"   í•™ìŠµ - ì†ì‹¤: {train_loss:.4f}, ì •í™•ë„: {train_acc:.3f}")
        print(f"   ê²€ì¦ - ì†ì‹¤: {val_loss:.4f}, ì •í™•ë„: {val_acc:.3f}")
        
        # ìµœê³  ëª¨ë¸ ì €ì¥
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            torch.save({
                'model_state_dict': model.state_dict(),
                'train_loss': train_loss,
                'val_loss': val_loss,
                'train_acc': train_acc,
                'val_acc': val_acc
            }, 'best_multimodal_lettuce_model.pth')
            print("   â­ ìƒˆë¡œìš´ ìµœê³  ëª¨ë¸ ì €ì¥!")
    
    print("âœ… ë©€í‹°ëª¨ë‹¬ í•™ìŠµ ì™„ë£Œ!")
    return model

# ============= ì‹¤ì‹œê°„ ì˜ˆì¸¡ í•¨ìˆ˜ =============
def predict_multimodal(model_path, image_path, planting_date, photo_date=None):
    """ì´ë¯¸ì§€ + ë‚ ì§œ ì •ë³´ë¡œ ì¢…í•© ì˜ˆì¸¡"""
    
    if photo_date is None:
        photo_date = datetime.now()
    
    # 1. ëª¨ë¸ ë¡œë“œ
    model = MultimodalLettuceModel()
    checkpoint = torch.load(model_path, map_location='cpu')
    model.load_state_dict(checkpoint['model_state_dict'])
    model.eval()
    
    # 2. ì´ë¯¸ì§€ ì „ì²˜ë¦¬
    val_transform = transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])
    
    image = Image.open(image_path).convert("RGB")
    image_tensor = val_transform(image).unsqueeze(0)
    
    # 3. ë‚ ì§œ ì •ë³´ ê³„ì‚°
    planting_date = pd.to_datetime(planting_date)
    photo_date = pd.to_datetime(photo_date)
    days_since_planting = (photo_date - planting_date).days
    days_to_harvest = max(0, 30 - days_since_planting)
    
    # ë‚ ì§œ ê¸°ë°˜ ì˜ˆì¸¡ ë‹¨ê³„
    if days_since_planting <= 7:
        predicted_stage_by_date = 0
    elif days_since_planting <= 21:
        predicted_stage_by_date = 1
    else:
        predicted_stage_by_date = 2
    
    # 4. ë©”íƒ€ë°ì´í„° ìƒì„±
    metadata = torch.tensor([[
        days_since_planting / 35.0,
        days_to_harvest / 35.0,
        predicted_stage_by_date / 2.0,
        1.0  # ì¼ì¹˜ë„ëŠ” ì˜ˆì¸¡ ì‹œ ì•Œ ìˆ˜ ì—†ìœ¼ë¯€ë¡œ 1.0ìœ¼ë¡œ ì„¤ì •
    ]], dtype=torch.float32)
    
    # 5. ì˜ˆì¸¡ ìˆ˜í–‰
    with torch.no_grad():
        stage_pred, days_pred, attention_weights = model(image_tensor, metadata)
        
        # ê²°ê³¼ í•´ì„
        stage_probs = F.softmax(stage_pred, dim=1)
        predicted_stage = torch.argmax(stage_probs, dim=1).item()
        stage_confidence = stage_probs[0][predicted_stage].item() * 100
        predicted_days = max(0, round(days_pred.item(), 1))
        
        vision_weight = attention_weights[0][0].item()
        metadata_weight = attention_weights[0][1].item()
        
        stage_names = ["ğŸŒ± ì •ì‹ê¸°", "ğŸŒ¿ ìƒìœ¡ê¸°", "ğŸ¯ ìˆ˜í™•ê¸°"]
        
        result = {
            "ì‹¬ì€_ë‚ ì§œ": planting_date.strftime("%Y-%m-%d"),
            "ì‚¬ì§„_ë‚ ì§œ": photo_date.strftime("%Y-%m-%d"),
            "ê²½ê³¼_ì¼ìˆ˜": days_since_planting,
            "ë‚ ì§œ_ê¸°ë°˜_ì˜ˆì¸¡": stage_names[predicted_stage_by_date],
            "AI_ì˜ˆì¸¡_ë‹¨ê³„": stage_names[predicted_stage],
            "ì˜ˆì¸¡_ì‹ ë¢°ë„": f"{stage_confidence:.1f}%",
            "ìˆ˜í™•_ì˜ˆìƒì¼": f"{predicted_days:.1f}ì¼ í›„",
            "ë¹„ì „_ê°€ì¤‘ì¹˜": f"{vision_weight:.2f}",
            "ë©”íƒ€ë°ì´í„°_ê°€ì¤‘ì¹˜": f"{metadata_weight:.2f}",
            "ì¢…í•©_íŒë‹¨": get_comprehensive_recommendation(
                days_since_planting, predicted_stage_by_date, predicted_stage, stage_confidence
            )
        }
        
        return result

def get_comprehensive_recommendation(days_elapsed, date_stage, ai_stage, confidence):
    """ë‚ ì§œì™€ AI ì˜ˆì¸¡ì„ ì¢…í•©í•œ ì¶”ì²œ"""
    
    if abs(date_stage - ai_stage) >= 2:
        return "âš ï¸ ë‚ ì§œì™€ AI ì˜ˆì¸¡ì´ í¬ê²Œ ë‹¤ë¦…ë‹ˆë‹¤. í™˜ê²½ì„ í™•ì¸í•´ì£¼ì„¸ìš”."
    elif confidence < 70:
        return "â“ ì˜ˆì¸¡ ì‹ ë¢°ë„ê°€ ë‚®ìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ê°ë„ì—ì„œ ì¬ì´¬ì˜í•´ë³´ì„¸ìš”."
    elif ai_stage == 2:
        return "ğŸ‰ ìˆ˜í™• ì‹œê¸°ì…ë‹ˆë‹¤!"
    elif days_elapsed > 25:
        return "ğŸ”” ìˆ˜í™• ì‹œê¸°ê°€ ë‹¤ê°€ì™”ìŠµë‹ˆë‹¤."
    else:
        return "âœ… ê±´ê°•í•˜ê²Œ ì„±ì¥ ì¤‘ì…ë‹ˆë‹¤."

# ============= ë©”ì¸ ì‹¤í–‰ =============
if __name__ == "__main__":
    print("ğŸ¥¬ ë©€í‹°ëª¨ë‹¬ ìŠ¤ë§ˆíŠ¸ ìƒì¶” ì˜ˆì¸¡ ì‹œìŠ¤í…œ")
    print("=" * 50)
    
    if not os.path.exists(CSV_PATH):
        print(f"âŒ CSV íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {CSV_PATH}")
    else:
        print(f"âœ… CSV íŒŒì¼ ë°œê²¬: {CSV_PATH}")
        
        # í•™ìŠµ ì‹¤í–‰
        model = train_multimodal_model()
        
        print("\nğŸ‰ í•™ìŠµ ì™„ë£Œ!")
        print("ğŸ“ ìƒì„±ëœ íŒŒì¼: best_multimodal_lettuce_model.pth")
        
        # í…ŒìŠ¤íŠ¸ ì˜ˆì¸¡ ì˜ˆì‹œ
        # result = predict_multimodal(
        #     'best_multimodal_lettuce_model.pth',
        #     'test_image.jpg',
        #     '2024-06-01',  # ì‹¬ì€ ë‚ ì§œ
        #     '2024-06-15'   # ì‚¬ì§„ ë‚ ì§œ
        # )
        # print(f"\nğŸ” ì˜ˆì¸¡ ê²°ê³¼:")
        # for key, value in result.items():
        #     print(f"   {key}: {value}")